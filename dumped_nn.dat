layers 6
layer 0 Dense
8 10
[-0.0237211  -0.0750738   0.08868445 -0.11652669  0.1322586  -0.15262565
 -0.07285851  0.09673172 -0.15976053 -0.12154002]
[-0.00707444 -0.04493186  0.05151585 -0.06926466  0.04388388  0.02067718
  0.03927648  0.03026477 -0.07327339  0.00680187]
[ 0.03917791  0.049916   -0.02301573  0.00054177 -0.01184011  0.03916648
  0.04777128  0.01395478  0.02230108  0.03406471]
[ 4.6066124e-02  3.2729033e-02 -1.5574426e-03 -7.7240020e-03
  7.8076012e-03 -4.7171779e-02  7.0281443e-03  6.1664308e-05
 -4.3997549e-02 -3.4702674e-02]
[ 0.03140472 -0.0168901  -0.0026038  -0.00348937 -0.00182714 -0.03114489
  0.02737456  0.01312403  0.01869217  0.00577093]
[-0.03140034 -0.00119309 -0.03093596  0.00098316  0.06032025 -0.04123278
 -0.00179569  0.04538863 -0.05374831  0.00203001]
[-0.16466044 -0.18310411  0.12533692 -0.0794521   0.1834341  -0.15034679
 -0.12792216  0.14467712 -0.15937059 -0.06207076]
[ 0.01049573  0.02229799  0.01232948 -0.05271345 -0.00494415 -0.06508742
  0.0820296   0.03611339 -0.01008846 -0.07387251]
[ 0.21359682  0.24736741 -0.19691364  0.12326608 -0.2108109   0.23539168
  0.22125424 -0.18413208  0.19368155  0.04992144]
layer 1 Activation
linear
layer 2 Dense
10 9
[ 0.17193581 -0.03814503  0.41261283 -0.33338076 -0.28135434  0.04395283
  0.0767895  -0.19830409 -0.3871476 ]
[ 0.49560836  0.16248778  0.4097653  -0.18153605 -0.4942026  -0.44015765
  0.5654366  -0.4169291  -0.02702639]
[ 0.02515423  0.14803189 -0.54785866  0.42182752  0.4502155   0.5229202
 -0.6117758   0.2299028  -0.21986873]
[ 0.12481514  0.47195372  0.28867105 -0.16474141  0.48676294 -0.48898122
  0.31211987 -0.15156919  0.26909468]
[ 0.13292405  0.18025368 -0.5215179  -0.46266162 -0.3447256   0.5095611
 -0.28553808 -0.22060475  0.4688829 ]
[ 0.035209   -0.17818303 -0.36428168 -0.3628952  -0.27435955 -0.44708884
 -0.4233887   0.00325098 -0.13353874]
[-0.4243778  -0.10159519 -0.06486008 -0.2507975  -0.3853284  -0.5687286
  0.07783981 -0.54807     0.20102617]
[ 0.54230326 -0.10535204 -0.28054816 -0.39191115  0.39241493  0.4249391
 -0.44470948 -0.07284512 -0.02478273]
[-0.14010173  0.31906337  0.0830182   0.27340174 -0.21805611  0.07553747
 -0.3662081   0.36084494 -0.55494446]
[-0.29416    -0.41169837 -0.4750341  -0.01801853 -0.3235874  -0.42940804
 -0.37471443  0.0422457   0.3813586 ]
[ 0.16772388 -0.02563932 -0.01543641  0.00702624 -0.22766496 -0.20391466
 -0.06674798  0.00792039 -0.22662084]
layer 3 Activation
relu
layer 4 Dense
9 1
[-0.01303394]
[0.0062133]
[0.02294512]
[0.03421622]
[0.0049233]
[0.04600834]
[0.10752492]
[-0.02650239]
[0.04696309]
[-0.1985475]
layer 5 Activation
linear
